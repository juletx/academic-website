@article{jumelet2025babybabellm,
 abstract = {We present BabyBabelLM, a multilingual collection of datasets modeling the language a person observes from birth until they acquire a native language. We curate developmentally plausible pretraining data aiming to cover the equivalent of 100M English words of content in each of 45 languages. We compile evaluation suites and train baseline models in each language. BabyBabelLM aims to facilitate multilingual pretraining and cognitive modeling.},
 archiveprefix = {arXiv},
 author = {Jaap Jumelet and Abdellah Fourtassi and Akari Haga and Bastian Bunzeck and Bhargav Shandilya and Diana Galvan-Sosa and Faiz Ghifari Haznitrama and Francesca Padovani and Francois Meyer and Hai Hu and Julen Etxaniz and others},
 eprint = {2510.10159},
 journal = {EACL 2026},
 keywords = {Natural Language Processing, Language Models, Deep Learning, Multilinguality, Cognitive Modeling},
 primaryclass = {cs.CL},
 title = {BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data},
 url = {https://arxiv.org/abs/2510.10159},
 year = {2025}
}
