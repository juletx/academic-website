---
title: Lessons from the Trenches on Reproducible Evaluation of Language Models

# Authors
# A YAML list of author names
# If you created a profile for a user (e.g. the default `admin` user at `content/authors/admin/`), 
# write the username (folder name) here, and it will be replaced with their full name and linked to their profile.
authors:
- Stella Biderman
- Hailey Schoelkopf
- Lintang Sutawika
- Leo Gao
- Jonathan Tow
- Baber Abbasi
- Alham Fikri Aji
- Pawan Sasanka Ammanamanchi
- Sidney Black
- Jordan Clive
- Anthony DiPofi
- Julen Etxaniz
- Benjamin Fattori
- Jessica Zosa Forde
- Charles Foster
- Jeffrey Hsu
- Mimansa Jaiswal
- Wilson Y. Lee
- Haonan Li
- Charles Lovering
- Niklas Muennighoff
- Ellie Pavlick
- Jason Phang
- Aviya Skowron
- Samson Tan
- Xiangru Tang
- Kevin A. Wang
- Genta Indra Winata
- Fran√ßois Yvon
- Andy Zou

# Author notes (such as 'Equal Contribution')
# A YAML list of notes for each author in the above `authors` list
author_notes: []

date: '2024-05-23'

# Date to publish webpage (NOT necessarily Bibtex publication's date).
publishDate: '2025-01-03T18:58:39.104447Z'

# Publication type.
# A single CSL publication type but formatted as a YAML list (for Hugo requirements).
publication_types:
- article-journal

# Publication name and optional abbreviated publication name.
publication: 'ArXiv'
publication_short: ''

doi: ''

abstract: 'Effective evaluation of language models remains an open challenge in NLP.
  Researchers and engineers face methodological issues such as the sensitivity of
  models to evaluation setup, difficulty of proper comparisons across methods, and
  the lack of reproducibility and transparency. In this paper we draw on three years
  of experience in evaluating large language models to provide guidance and lessons
  for researchers. First, we provide an overview of common challenges faced in language
  model evaluation. Second, we delineate best practices for addressing or lessening
  the impact of these challenges on research. Third, we present the Language Model
  Evaluation Harness (lm-eval): an open source library for independent, reproducible,
  and extensible evaluation of language models that seeks to address these issues.
  We describe the features of the library as well as case studies in which the library
  has been used to alleviate these methodological concerns.'

# Summary. An optional shortened abstract.
summary: ''

tags:
- Natural Language Processing
- Large Language Models
- Deep Learning
- Evaluation
- Reproducibility

# Display this page in a list of Featured pages?
featured: true

# Links
url_pdf: https://arxiv.org/pdf/2405.14782
url_code: https://github.com/EleutherAI/lm-evaluation-harness
url_dataset: ''
url_poster: ''
url_project: ''
url_slides: ''
url_source: ''
url_video: ''

# Custom links (uncomment lines below)
# links:
# - name: Custom Link
#   url: http://example.org

# Publication image
# Add an image named `featured.jpg/png` to your page's folder then add a caption below.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Associated Projects (optional).
#   Associate this publication with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects: ['internal-project']` links to `content/project/internal-project/index.md`.
#   Otherwise, set `projects: []`.
projects: []
links:
- name: arXiv
  url: https://arxiv.org/abs/2405.14782
---
