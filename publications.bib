@thesis{etxaniz2021prometa,
  title    = {ProMeta: softwarearen garapenerako prozesuen definizio eta ezarpenerako sistema metaereduetan oinarrituta},
  author   = {Julen Etxaniz},
  year     = {2021},
  date     = {2021-10-08},
  url      = {https://addi.ehu.es/handle/10810/53310},
  journal  = {ADDI},
  abstract = {The objective of the project is to build a system for the definition and implementation of software development processes based on metamodels. In fact, there are several methodologies that are suitable for software development. It is important to define the information of these methodologies through models so that they can be managed flexibly in the future and improvements can be made. In addition, it is necessary to build a system that establishes a methodology using information from the model for use by development teams in projects. The OpenUP methodology was used for the development of the project and the CCII-N2016-02 standard for the drafting of the project documentation and memory.},
  keywords = {Software Engineering, Web Development}
}

@thesis{etxaniz2023grounding,
  title    = {Grounding Language Models for Compositional and Spatial Reasoning},
  author   = {Julen Etxaniz and Oier Lopez de Lacalle and Aitor Soroa},
  year     = {2023},
  date     = {2023-06-30},
  url      = {https://addi.ehu.es/handle/10810/61827},
  journal  = {ADDI},
  abstract = {Humans can learn to understand and process the distribution of space, and one of the initial tasks of Artificial Intelligence has been to show machines the relationships between space and the objects that appear in it. Humans naturally combine vision and textual information to acquire compositional and spatial relationships among objects, and when reading a text, we are able to mentally depict the spatial relationships that may appear in it. Thus, the visual differences between images depicting "a person sits and a dog stands" and "a person stands and a dog sits" are obvious for humans, but still not clear for automatic systems.

              In this project, we propose to evaluate grounded Neural Language models that can perform compositional and spatial reasoning. Neural Language models (LM) have shown impressive capabilities on many NLP tasks but, despite their success, they have been criticized for their lack of meaning. Vision-and-Language models (VLM), trained jointly on text and image data, have been offered as a response to such criticisms, but recent work has shown that these models struggle to ground spatial concepts properly. In the project, we evaluate state-of-the-art pre-trained and fine-tuned VLMs to understand their grounding level on compositional and spatial reasoning. We also propose a variety of methods to create synthetic datasets specially focused on compositional reasoning.

              We managed to accomplish all the objectives of this work. First, we improved the state-of-the-art in compositional reasoning. Next, we performed some zero-shot experiments on spatial reasoning. Finally, we explored three alternatives for synthetic dataset creation: text-to-image generation, image captioning and image retrieval. Code is released at https://github.com/juletx/spatial-reasoning and models are released at https://huggingface.co/juletxara.},
  keywords = {Artificial Intelligence, Deep Learning, Natural Language Processing, Computer Vision, Grounding, Visual Reasoning, Compositional Reasoning, Spatial Reasoning}
}

@article{etxaniz2023multilingual,
  title         = {Do Multilingual Language Models Think Better in English?},
  author        = {Julen Etxaniz and Gorka Azkune and Aitor Soroa and Oier Lopez de Lacalle and Mikel Artetxe},
  booktitle     = {NAACL 2024},
  year          = {2023},
  date          = {2023-08-02},
  eprint        = {2308.01223},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  journal       = {arXiv},
  abstract      = {Translate-test is a popular technique to improve the performance of multilingual language models. This approach works by translating the input into English using an external machine translation system, and running inference over the translated input. However, these improvements can be attributed to the use of a separate translation system, which is typically trained on large amounts of parallel data not seen by the language model. In this work, we introduce a new approach called self-translate, which overcomes the need of an external translation system by leveraging the few-shot translation capabilities of multilingual language models. Experiments over 5 tasks show that self-translate consistently outperforms direct inference, demonstrating that language models are unable to leverage their full multilingual potential when prompted in non-English languages. Our code is available at https://github.com/juletx/self-translate.},
  keywords      = {Natural Language Processing, Large Language Models, Deep Learning, Multilinguality}
}

@article{sainz2023nlp,
  title         = {NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark},
  author        = {Oscar Sainz and Jon Ander Campos and Iker García-Ferrero and Julen Etxaniz and Oier Lopez de Lacalle and Eneko Agirre},
  booktitle     = {EMNLP 2023 Findings},
  year          = {2023},
  date          = {2023-10-27},
  eprint        = {2310.18018},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.},
  keywords      = {Natural Language Processing, Large Language Models, Evaluation, Data Contamination, Deep Learning}
}

@article{etxaniz2024latxa,
  title         = {Latxa: An Open Language Model and Evaluation Suite for Basque},
  author        = {Julen Etxaniz and Oscar Sainz and Naiara Perez and Itziar Aldabe and German Rigau and Eneko Agirre and Aitor Ormazabal and Mikel Artetxe and Aitor Soroa},
  booktitle     = {ACL 2024},
  year          = {2024},
  date          = {2024-03-29},
  eprint        = {2403.20266},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses at https://github.com/hitz-zentroa/latxa. Our suite enables reproducible research on methods to build LLMs for low-resource languages.},
  keywords      = {Natural Language Processing, Large Language Models, Deep Learning, Multilinguality, Basque}
}

@article{heredia2024xnlieu,
  title         = {XNLIeu: a dataset for cross-lingual NLI in Basque},
  author        = {Maite Heredia and Julen Etxaniz and Muitze Zulaika and Xabier Saralegi and Jeremy Barnes and Aitor Soroa},
  booktitle     = {NAACL 2024},
  year          = {2024},
  date          = {2024-04-10},
  eprint        = {2404.06996},
  archiveprefix = {arXiv},
  primaryclass  = {cs.CL},
  abstract      = {XNLI is a popular Natural Language Inference (NLI) benchmark widely used to evaluate cross-lingual Natural Language Understanding (NLU) capabilities across languages. In this paper, we expand XNLI to include Basque, a low-resource language that can greatly benefit from transfer-learning approaches. The new dataset, dubbed XNLIeu, has been developed by first machine-translating the English XNLI corpus into Basque, followed by a manual post-edition step. We have conducted a series of experiments using mono- and multilingual LLMs to assess a) the effect of professional post-edition on the MT system; b) the best cross-lingual strategy for NLI in Basque; and c) whether the choice of the best cross-lingual strategy is influenced by the fact that the dataset is built by translation. The results show that post-edition is necessary and that the translate-train cross-lingual strategy obtains better results overall, although the gain is lower when tested in a dataset that has been built natively from scratch. Our code and datasets are publicly available under open licenses at https://github.com/hitz-zentroa/xnli-eu.},
  keywords      = {Natural Language Processing, Large Language Models, Deep Learning, Multilinguality, Basque}
}

@article{agirre2024ikergaitu,
  title     = {IKER-GAITU: research on language technology for Basque and other low-resource languages},
  author    = {Agirre, Eneko and Aldabe, Itziar and Arregi, Xabier and Artetxe, Mikel and Atutxa, Unai and Azurmendi, Ekhi and De la Iglesia, Iker and Etxaniz, Julen and García-Romillo, Victor and Hernaez-Rioja, Inma and others},
  year      = {2024},
  date      = {2024-04-15},
  booktitle = {PROJECTS & DEMOS SEPLN - CEDI 2024},
  abstract  = {The general objective of the IKER-GAITU project is to research on language technology to increase the presence of Basque in the digital environment. It will be carried out between 2023 and 2025 thanks to a grant from the Department of Culture and Language Policy of the Basque Government. Current techniques require enormous amounts of textual and oral data per language. On the other hand, the data available for Basque and other low-resource languages might not be enough to attain the same quality as larger languages with the current technology. For this reason, it is essential to research on language technology, so that low-resource languages are present with the same quality as the rest of the languages in these technologies. IKER-GAITU pursues the following research objectives: 1. A system that automatically captures the level of Basque proficiency, written and oral; 2. Bring pSersonalized voice technology to people with disabilities; 3. Spontaneous voice transcription, both when Basque and Spanish are mixed and when there are several speakers; 4. Textual conversational systems in Basque that match the quality of the most powerful large language models. In this project summary we present the results for the first year. More information at https://hitz.eus/iker-gaitu.},
  keywords  = {Natural Language Processing, Large Language Models, Deep Learning, Multilinguality, Basque}
}

@article{biderman2024lmevaluation,
  title={Lessons from the Trenches on Reproducible Evaluation of Language Models},
  author={Stella Biderman and Hailey Schoelkopf and Lintang Sutawika and Leo Gao and Jonathan Tow and Baber Abbasi and Alham Fikri Aji and Pawan Sasanka Ammanamanchi and Sidney Black and Jordan Clive and Anthony DiPofi and Julen Etxaniz and Benjamin Fattori and Jessica Zosa Forde and Charles Foster and Jeffrey Hsu and Mimansa Jaiswal and Wilson Y. Lee and Haonan Li and Charles Lovering and Niklas Muennighoff and Ellie Pavlick and Jason Phang and Aviya Skowron and Samson Tan and Xiangru Tang and Kevin A. Wang and Genta Indra Winata and François Yvon and Andy Zou},
  year={2024},
  date={2024-05-23},
  eprint={2405.14782},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2405.14782},
  abstract={Effective evaluation of language models remains an open challenge in NLP. Researchers and engineers face methodological issues such as the sensitivity of models to evaluation setup, difficulty of proper comparisons across methods, and the lack of reproducibility and transparency. In this paper we draw on three years of experience in evaluating large language models to provide guidance and lessons for researchers. First, we provide an overview of common challenges faced in language model evaluation. Second, we delineate best practices for addressing or lessening the impact of these challenges on research. Third, we present the Language Model Evaluation Harness (lm-eval): an open source library for independent, reproducible, and extensible evaluation of language models that seeks to address these issues. We describe the features of the library as well as case studies in which the library has been used to alleviate these methodological concerns.},
  keywords={Natural Language Processing, Large Language Models, Deep Learning, Evaluation, Reproducibility}
}

@article{etxaniz2024bertaqa,
  title={BertaQA: How Much Do Language Models Know About Local Culture?},
  author={Julen Etxaniz and Gorka Azkune and Aitor Soroa and Oier Lopez de Lacalle and Mikel Artetxe},
  year={2024},
  date={2024-06-11},
  booktitle = {NeurIPS Datasets and Benchmarks 2024},
  eprint={2406.07302},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2406.07302},
  abstract={Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.},
  keywords={Natural Language Processing, Large Language Models, Deep Learning, Evaluation, Multilinguality, Culture, Basque}
}

@article{perez2024latxa,
  title={Latxa Euskarazko Hizkuntza-Eredua},
  author={Perez, Naiara and Etxaniz, Julen and Sainz, Oscar and Aldabe, Itziar and Rigau, German and Agirre, Eneko and Salem, Ahmed and Ormazabal, Aitor and Artetxe, Mikel and Soroa, Aitor},
  journal={EKAIA EHUko Zientzia eta Teknologia aldizkaria},
  year={2024},
  date={2024-09-24},
  abstract={Artikulu honetan Latxa hizkuntza-ereduak (HE) aurkeztuko ditugu, egun euskararako garatu diren HE handienak. Latxa HEek 7.000 miloi parametrotik 70.000 milioira bitartean dituzte, eta ingeleseko LLama 2 ereduetatik eratorriak dira. Horretarako, LLama 2 gainean aurreikasketa jarraitua izeneko prozesua gauzatu da, 4.3 milioi dokumentu eta 4.200 milioi token duen euskarazko corpusa erabiliz. Euskararentzat kalitate handiko ebaluazio multzoen urritasunari aurre egiteko, lau ebaluazio multzo berri bildu ditugu: EusProficiency, EGA azterketaren atariko frogako 5.169 galdera biltzen dituena; EusReading, irakurketaren ulermeneko 352 galdera biltzen dituena; EusTrivia, 5 arlotako ezagutza orokorreko 1.715 galdera biltzen dituena; eta EusExams, oposizioetako 16.774 galdera biltzen dituena. Datu-multzo berri hauek erabiliz, Latxa eta beste euskarazko HEak ebaluatu ditugu (elebakar zein eleanitzak), eta esperimentuek erakusten dute Latxak aurreko eredu ireki guztiak gainditzen dituela. Halaber, GPT-4 Turbo HE komertzialarekiko emaitza konpetitiboak lortzen ditu Latxak, hizkuntza-ezagutzan eta ulermenean, testu-irakurmenean zein ezagutza intentsiboa eskatzen duten atazetan atzeratuta egon arren. Bai Latxa ereduen familia, baita gure corpus eta ebaluazio-datu berriak ere lizentzia irekien pean daude publiko https://github. com/hitz-zentroa/latxa helbidean.},
  keywords={Natural Language Processing, Large Language Models, Deep Learning, Multilinguality, Basque}
}

@article{pensa2024gita4calamita,
  title={GITA4CALAMITA - Evaluating the Physical Commonsense Understanding of Italian LLMs in a Multi-layered Approach: A CALAMITA Challenge},
  author={Pensa, Giulia and Azurmendi, Ekhi and Etxaniz, Julen and Altuna, Bego{\~n}a and Gonzalez-Dios, Itziar},
  year={2024},
  date={2024-12-06},
  booktitle = {CLiC-it 2024},
  url = {https://aclanthology.org/2024.clicit-1.127/},
  abstract = {In the context of the CALAMITA Challenge, we investigate the physical commonsense reasoning capabilities of large language models (LLMs) and introduce a methodology to assess their understanding of the physical world. To this end, we use a test set designed to evaluate physical commonsense reasoning in LLMs for the Italian language. We present a tiered dataset,
  named the Graded Italian Annotated dataset (GITA), which is written and annotated by a professional linguist. This dataset
  enables us to focus on three distinct levels of commonsense understanding. Our benchmark aims to evaluate three specific
  tasks: identifying plausible and implausible stories within our dataset, identifying the conflict that generates an implausible
  story, and identifying the physical states that make a story implausible. We perform these tasks using LLAMA3, Gemma2
  and Mistral. Our findings reveal that, although the models may excel at high-level classification tasks, their reasoning is
  inconsistent and unverifiable, as they fail to capture intermediate evidence.},
  keywords = {Natural Language Processing, Large Language Models, Deep Learning, Evaluation, Commonsense Reasoning, Italian}
}

@article{bengoetxea2024hitzvardial,
  title={HiTZ at VarDial 2025 NorSID: Overcoming Data Scarcity with Language Transfer and Automatic Data Annotation},
  author={Jaione Bengoetxea and Mikel Zubillaga and Ekhi Azurmendi and Maite Heredia and Julen Etxaniz and Markel Ferro and Jeremy Barnes},
  year={2024},
  date={2024-12-13},
  eprint={2412.10095},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  booktitle = {COLING 2025},
  url={https://aclanthology.org/2025.vardial-1.16/},
  abstract={In this paper we present our submission for the NorSID Shared Task as part of the 2025 VarDial Workshop (Scherrer et al., 2025), consisting of three tasks: Intent Detection, Slot Filling and Dialect Identification, evaluated using data in different dialects of the Norwegian language. For Intent Detection and Slot Filling, we have fine-tuned a multitask model in a cross-lingual setting, to leverage the xSID dataset available in 17 languages. In the case of Dialect Identification, our final submission consists of a model fine-tuned on the provided development set, which has obtained the highest scores within our experiments. Our final results on the test set show that our models do not drop in performance compared to the development set, likely due to the domain-specificity of the dataset and the similar distribution of both subsets. Finally, we also report an in-depth analysis of the provided datasets and their artifacts, as well as other sets of experiments that have been carried out but did not yield the best results. Additionally, we present an analysis on the reasons why some methods have been more successful than others; mainly the impact of the combination of languages and domain-specificity of the training data on the results.},
  keywords={Natural Language Processing, Large Language Models, Deep Learning, Multilinguality, Dialects, Norwegian}
}

@article{calvo2025truth,
  title = "Truth Knows No Language: Evaluating Truthfulness Beyond {E}nglish",
  author = "Calvo Figueras, Blanca  and
    Sagarzazu, Eneko  and
    Etxaniz, Julen  and
    Barnes, Jeremy  and
    Gamallo, Pablo  and
    de-Dios-Flores, Iria  and
    Agerri, Rodrigo",
  editor = "Che, Wanxiang  and
    Nabende, Joyce  and
    Shutova, Ekaterina  and
    Pilehvar, Mohammad Taher",
  booktitle = "ACL 2025",
  month = jul,
  year = "2025",
  address = "Vienna, Austria",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2025.acl-long.1507/",
  doi = "10.18653/v1/2025.acl-long.1507",
  pages = "31204--31218",
  ISBN = "979-8-89176-251-0",
  abstract = "We introduce a professionally translated extension of the TruthfulQA benchmark designed to evaluate truthfulness in Basque, Catalan, Galician, and Spanish. Truthfulness evaluations of large language models (LLMs) have primarily been focused on English. However, the ability of LLMs to maintain truthfulness across languages remains under-explored. Our study evaluates 12 state-of-the-art open LLMs, comparing base and instruction-tuned models using human evaluation, multiple-choice metrics, and LLM-as-a-Judge scoring. Our findings reveal that, while LLMs perform best in English and worst in Basque (the lowest-resourced language), overall truthfulness discrepancies across languages are smaller than anticipated. Furthermore, we show that LLM-as-a-Judge correlates more closely with human judgments than multiple-choice metrics, and that informativeness plays a critical role in truthfulness assessment. Our results also indicate that machine translation provides a viable approach for extending truthfulness benchmarks to additional languages, offering a scalable alternative to professional translation. Finally, we observe that universal knowledge questions are better handled across languages than context- and time-dependent ones, highlighting the need for truthfulness evaluations that account for cultural and temporal variability. Datasets, models and code are publicly available under open licenses.",
  keywords = {Natural Language Processing, Large Language Models, Deep Learning, Multilinguality, Truthfulness, Evaluation}
}

@article{sainz2025instructing,
  title = "Instructing Large Language Models for Low-Resource Languages: A Systematic Study for {B}asque",
  author = "Sainz, Oscar  and
    Perez, Naiara  and
    Etxaniz, Julen  and
    Fernandez de Landa, Joseba  and
    Aldabe, Itziar  and
    Garc{\'i}a-Ferrero, Iker  and
    Zabala, Aimar  and
    Azurmendi, Ekhi  and
    Rigau, German  and
    Agirre, Eneko  and
    Artetxe, Mikel  and
    Soroa, Aitor",
  editor = "Christodoulopoulos, Christos  and
    Chakraborty, Tanmoy  and
    Rose, Carolyn  and
    Peng, Violet",
  booktitle = "EMNLP 2025",
  month = nov,
  year = "2025",
  address = "Suzhou, China",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2025.emnlp-main.1484/",
  doi = "10.18653/v1/2025.emnlp-main.1484",
  pages = "29136--29160",
  ISBN = "979-8-89176-332-6",
  abstract = "Instructing language models with user intent requires large instruction datasets, which are only available for a limited set of languages. In this paper, we explore alternatives to conventional instruction adaptation pipelines in low-resource scenarios. We assume a realistic scenario for low-resource languages, where only the following are available: corpora in the target language, existing open-weight multilingual base and instructed backbone LLMs, and synthetically generated instructions sampled from the instructed backbone. We present a comprehensive set of experiments for Basque that systematically study different combinations of these components evaluated on benchmarks and human preferences from 1,680 participants. Our conclusions show that target language corpora are essential, with synthetic instructions yielding robust models, and, most importantly, that using as backbone an instruction-tuned model outperforms using a base non-instructed model. Scaling up to Llama 3.1 Instruct 70B as backbone, our model comes near frontier models of much larger sizes for Basque, without using any Basque instructions. We release code, models, instruction datasets, and human preferences to support full reproducibility in future research on low-resource language adaptation.",
  keywords = {Natural Language Processing, Large Language Models, Deep Learning, Multilinguality, Basque, Instruction Tuning}
}

@article{jumelet2025babybabellm,
  title={BabyBabelLM: A Multilingual Benchmark of Developmentally Plausible Training Data},
  author={Jaap Jumelet and Abdellah Fourtassi and Akari Haga and Bastian Bunzeck and Bhargav Shandilya and Diana Galvan-Sosa and Faiz Ghifari Haznitrama and Francesca Padovani and Francois Meyer and Hai Hu and Julen Etxaniz and others},
  journal={EACL 2026},
  year={2025},
  eprint={2510.10159},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2510.10159},
  abstract={We present BabyBabelLM, a multilingual collection of datasets modeling the language a person observes from birth until they acquire a native language. We curate developmentally plausible pretraining data aiming to cover the equivalent of 100M English words of content in each of 45 languages. We compile evaluation suites and train baseline models in each language. BabyBabelLM aims to facilitate multilingual pretraining and cognitive modeling.},
  keywords = {Natural Language Processing, Language Models, Deep Learning, Multilinguality, Cognitive Modeling}
}

@article{arana2025multimodal,
  title={Multimodal Large Language Models for Low-Resource Languages: A Case Study for Basque},
  author={Lukas Arana and Julen Etxaniz and Ander Salaberria and Gorka Azkune},
  journal={arXiv},
  year={2025},
  eprint={2511.09396},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2511.09396},
  abstract={Current Multimodal Large Language Models exhibit very strong performance for several demanding tasks. While commercial MLLMs deliver acceptable performance in low-resource languages, comparable results remain unattained within the open science community. In this paper, we aim to develop a strong MLLM for a low-resource language, namely Basque. For that purpose, we develop our own training and evaluation image-text datasets. Using two different Large Language Models as backbones, the Llama-3.1-Instruct model and a Basque-adapted variant called Latxa, we explore several data mixtures for training. We show that: i) low ratios of Basque multimodal data (around 20%) are already enough to obtain solid results on Basque benchmarks, and ii) contrary to expected, a Basque instructed backbone LLM is not required to obtain a strong MLLM in Basque. Our results pave the way to develop MLLMs for other low-resource languages by openly releasing our resources.},
  keywords = {Natural Language Processing, Large Language Models, Deep Learning, Evaluation, Multilinguality, Basque, Multimodal}
}

@article{azurmendi2025bernat,
  title={BERnaT: Basque Encoders for Representing Natural Textual Diversity},
  author={Ekhi Azurmendi and Joseba Fernandez de Landa and Jaione Bengoetxea and Maite Heredia and Julen Etxaniz and Mikel Zubillaga and Ander Soraluze and Aitor Soroa},
  journal={arXiv},
  year={2025},
  eprint={2512.03903},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2512.03903},
  abstract={Language models depend on massive text corpora that are often filtered for quality, a process that can unintentionally exclude non-standard linguistic varieties, reduce model robustness and reinforce representational biases. In this paper, we argue that language models should aim to capture the full spectrum of language variation (dialectal, historical, informal, etc.) rather than relying solely on standardized text. Focusing on Basque, a morphologically rich and low-resource language, we construct new corpora combining standard, social media, and historical sources, and pre-train the BERnaT family of encoder-only models in three configurations: standard, diverse, and combined. We further propose an evaluation framework that separates Natural Language Understanding (NLU) tasks into standard and diverse subsets to assess linguistic generalization. Results show that models trained on both standard and diverse data consistently outperform those trained on standard corpora, improving performance across all task types without compromising standard benchmark accuracy. These findings highlight the importance of linguistic diversity in building inclusive, generalizable language models.},
  keywords = {Natural Language Processing, Large Language Models, Deep Learning, Evaluation, Multilinguality, Basque, Linguistic Diversity}
}

@article{nissim2025challenging,
  title={Challenging the Abilities of Large Language Models in Italian: a Community Initiative},
  author={Nissim, Malvina and Croce, Danilo and Patti, Viviana and Basile, Pierpaolo and Attanasio, Giuseppe and Musacchio, Elio and Rinaldi, Matteo and Borazio, Federico and Francis, Maria and Gili, Jacopo and others},
  journal={arXiv},
  year={2025},
  eprint={2512.04759},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  keywords = {Natural Language Processing, Large Language Models, Deep Learning, Evaluation, Commonsense Reasoning, Italian},
  abstract={The rapid progress of Large Language Models (LLMs) has transformed natural language processing and broadened its impact across research and society. Yet, systematic evaluation of these models, especially for languages beyond English, remains limited. "Challenging the Abilities of LAnguage Models in ITAlian" (CALAMITA) is a large-scale collaborative benchmarking initiative for Italian, coordinated under the Italian Association for Computational Linguistics. Unlike existing efforts that focus on leaderboards, CALAMITA foregrounds methodology: it federates more than 80 contributors from academia, industry, and the public sector to design, document, and evaluate a diverse collection of tasks, covering linguistic competence, commonsense reasoning, factual consistency, fairness, summarization, translation, and code generation. Through this process, we not only assembled a benchmark of over 20 tasks and almost 100 subtasks, but also established a centralized evaluation pipeline that supports heterogeneous datasets and metrics. We report results for four open-weight LLMs, highlighting systematic strengths and weaknesses across abilities, as well as challenges in task-specific evaluation. Beyond quantitative results, CALAMITA exposes methodological lessons: the necessity of fine-grained, task-representative metrics, the importance of harmonized pipelines, and the benefits and limitations of broad community engagement. CALAMITA is conceived as a rolling benchmark, enabling continuous integration of new tasks and models. This makes it both a resource -- the most comprehensive and diverse benchmark for Italian to date -- and a framework for sustainable, community-driven evaluation. We argue that this combination offers a blueprint for other languages and communities seeking inclusive and rigorous LLM evaluation practices.},
  keywords = {Natural Language Processing, Large Language Models, Deep Learning, Evaluation, Italian}
}
